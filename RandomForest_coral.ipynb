{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning \n",
    "## The notebook will build a Random Forest Classifier model to investigate coral bleaching\n",
    "### This is an academic project from Dr. Anna Haywood machine learning workshop\n",
    "### The project aims to build a random forest to predict coral bleaching areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is from https://noaa.maps.arcgis.com/apps/instant/portfolio/index.html?appid=8dbe7b587c1d463bb2ad65e8b65da0d5. The imported data is from Fiji with 5 bleaching alert areas (0, 1, 2, 3, 4) with 0 indicating no bleaching and 5 is heavily bleaching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preview: \n",
      "          Date  Latitude  Longitude  Sea_Surface_Temperature  HotSpots  \\\n",
      "0  01/01/1985   -16.775     179.35                    27.80       0.0   \n",
      "1  01/02/1985   -16.775     179.35                    27.99       0.0   \n",
      "2  01/03/1985   -16.775     179.35                    28.06       0.0   \n",
      "3  01/04/1985   -16.775     179.35                    26.94       0.0   \n",
      "4  01/05/1985   -16.775     179.35                    27.03       0.0   \n",
      "\n",
      "   Degree_Heating_Weeks  Bleaching_Alert_Area  \n",
      "0                   0.0                     0  \n",
      "1                   0.0                     0  \n",
      "2                   0.0                     0  \n",
      "3                   0.0                     0  \n",
      "4                   0.0                     0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = '/Users/hangtran/Desktop/Project_4/Fiji (1).csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preview data\n",
    "print(\"Data preview: \\n\", data.head(), \"\\n\")\n",
    "required_columns = [\n",
    "    'Date', 'Latitude', 'Longitude',\n",
    "    'Sea_Surface_Temperature', 'HotSpots',\n",
    "    'Degree_Heating_Weeks',  'Bleaching_Alert_Area'\n",
    "]\n",
    "# Check missing columns\n",
    "for col in required_columns:\n",
    "    if col not in data.columns:\n",
    "        raise ValueError(f'Missing columns: {col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin to train a Random Forest Classifier with the target is the bleaching alert area mention above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.90844 \n",
      "\n",
      "Confusion matrix: \n",
      " [[1776   44    0    0    0]\n",
      " [ 134  647   21    3    6]\n",
      " [   2   41  140    0    0]\n",
      " [   0   10    1   57    1]\n",
      " [   0    6    0    0   49]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      1820\n",
      "           1       0.86      0.80      0.83       811\n",
      "           2       0.86      0.77      0.81       183\n",
      "           3       0.95      0.83      0.88        69\n",
      "           4       0.88      0.89      0.88        55\n",
      "\n",
      "    accuracy                           0.91      2938\n",
      "   macro avg       0.90      0.85      0.87      2938\n",
      "weighted avg       0.91      0.91      0.91      2938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split features (X) and target (y)\n",
    "X = data.drop(columns = ['Date', 'Bleaching_Alert_Area'])\n",
    "y = data['Bleaching_Alert_Area']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy score: {accuracy: 0.5f} \\n')\n",
    "print(f'Confusion matrix: \\n {confusion}')\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result: \n",
    "The model is performing well with an accuracy of 91%. We have 5 classes (0, 1, 2, 3, 4) indicating 5 bleaching alert areas. \n",
    "\n",
    "For class 0, high precision (93%) and high recall (98%) meaning models capture all true positive case. \n",
    "\n",
    "For class 1, lower recall suggesting some of instance are misclassfied. \n",
    "\n",
    "For class 2,3, and 4, overall good precision and recall but their number of samples (support) are low. \n",
    "\n",
    "Now, we will begin to address issue of our model. There are 2 prominent issues\n",
    "1. Imbalance of classes with class 0 is overrepresented in the dataset\n",
    "2. Parameters is not optimized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing the imblance of classes with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleaching_Alert_Area\n",
      "0    9081\n",
      "1    3933\n",
      "2     998\n",
      "3     376\n",
      "4     300\n",
      "Name: count, dtype: int64\n",
      "Bleaching_Alert_Area\n",
      "0    7261\n",
      "1    7261\n",
      "3    7261\n",
      "2    7261\n",
      "4    7261\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Address class imbalance, first check the frequency of each class\n",
    "print(y.value_counts())\n",
    "# Class 0 is much larger than the other classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42, sampling_strategy='auto')\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(y_train_resampled.value_counts()) #Now, every class is balance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every class has been resampled by SMOTE, we can build the model again with this balanced data. Notice, we will not touch the test data since there will be data leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score after resample:  0.89346 \n",
      "\n",
      "Confusion matrix after resample: \n",
      " [[1742   77    1    0    0]\n",
      " [ 116  612   60    9   14]\n",
      " [   2   25  156    0    0]\n",
      " [   0    4    1   63    1]\n",
      " [   0    3    0    0   52]]\n",
      "Classification report after resample:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1820\n",
      "           1       0.85      0.75      0.80       811\n",
      "           2       0.72      0.85      0.78       183\n",
      "           3       0.88      0.91      0.89        69\n",
      "           4       0.78      0.95      0.85        55\n",
      "\n",
      "    accuracy                           0.89      2938\n",
      "   macro avg       0.83      0.88      0.85      2938\n",
      "weighted avg       0.89      0.89      0.89      2938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit model, predict and evaluation after SMOTE\n",
    "rf.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy score after resample: {accuracy: 0.5f} \\n')\n",
    "print(f'Confusion matrix after resample: \\n {confusion}')\n",
    "print(\"Classification report after resample:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result: \n",
    "Though the accuracy value is lower (89%) we can be assured that our model is now better at predict other classes and not biased toward class 0. \n",
    "\n",
    "SMOTE introduce synthetic values for underepresented classes forcing the model to learn better. \n",
    "\n",
    "The recall of class 2 and class 4 is improving meaning model is better at detecting minority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score after resample & hyperparameters tuning:  0.89381 \n",
      "\n",
      "Confusion matrix after resample & hyperparamters tuning: \n",
      " [[1748   71    1    0    0]\n",
      " [ 117  607   62   10   15]\n",
      " [   2   25  156    0    0]\n",
      " [   0    4    1   63    1]\n",
      " [   0    3    0    0   52]]\n",
      "Classification report after resample & hyperparameters tuning:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1820\n",
      "           1       0.85      0.75      0.80       811\n",
      "           2       0.71      0.85      0.77       183\n",
      "           3       0.86      0.91      0.89        69\n",
      "           4       0.76      0.95      0.85        55\n",
      "\n",
      "    accuracy                           0.89      2938\n",
      "   macro avg       0.83      0.88      0.85      2938\n",
      "weighted avg       0.89      0.89      0.89      2938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tunning \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "parameters = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'class_weight':['balanced', None]\n",
    "}\n",
    "rf_hyper = RandomForestClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    param_distributions=parameters,\n",
    "    estimator=rf_hyper,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted'\n",
    ")\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(f'Accuracy score after resample & hyperparameters tuning: {accuracy: 0.5f} \\n')\n",
    "print(f'Confusion matrix after resample & hyperparamters tuning: \\n {confusion}')\n",
    "print(\"Classification report after resample & hyperparameters tuning:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result: \n",
    "The accuracy value increase slightly (89.381%), the model performs better after tuning. \n",
    "\n",
    "The confusion matrix shows model performs well across all classes. \n",
    "\n",
    "Class 0 has higher of true positive meaning it is well classified\n",
    "\n",
    "Classes 1 and 2 still have noticeable misclassifications, but they are more balanced than before, likely due to the resampling technique.\n",
    "\n",
    "Classification Report: Class 0 still performs the best, but classes 1 and 3 show decent recall, meaning they are being identified more frequently.\n",
    "\n",
    "Class 4 has a high recall (0.95), meaning the model is very good at detecting it, though precision could still be better (0.76)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "Random Forest Model performs well with the accuracy of 89.381% after resample with SMOTE and hyperparameter tuning. \n",
    "\n",
    "The macro average shows a balanced performance across all classes and the weighted average confirms that model is doing well. \n",
    "\n",
    "However, the model has a high weighted F1-score (0.89) meaning the model is still favoring the larger classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step for future improvement:\n",
    "\n",
    "1. Further hyperparameter tuning\n",
    "2. Feature Engineering\n",
    "3. Advance resampling technique\n",
    "4. Ensemble models as XGBoost and LightGBM\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
